{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c0bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fec105",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'NewsCategorizer.csv'\n",
    "# the converters argument will let Python read the ability column as a list, not a string\n",
    "news = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13386559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "def headline_features(headline):\n",
    "    # stopwords list is all lowercase so we need to match\n",
    "    headline = headline.lower()\n",
    "    words = headline.split()\n",
    "    keywords = [w for w in words if w not in get_stop_words('english')]\n",
    "    while len(keywords) < 3:\n",
    "        keywords.append('None')\n",
    "    return {'first_keyword': keywords[0], 'second_keyword': keywords[1], 'third_keyword': keywords[2]}\n",
    "\n",
    "headline_features('High Tech Works When It Enables High Touch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479b4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dataset happens to already contain key words, but for new data (ie news not from this dataset) we'll most likely\n",
    "# have a headline and not a list of keywords\n",
    "news[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e697fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 categories, so our chances of guessing a headline correctly purely by chance is 10% <-- we want to do better!\n",
    "news.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e65c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_features = zip(news.headline, news.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7ac83",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(headline_features(headline), category) for headline, category in zipped_features ]\n",
    "featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3f5ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a57be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "print(len(featuresets))\n",
    "split_num = math.floor(len(featuresets)*.8)\n",
    "print(split_num)\n",
    "\n",
    "# split feature sets into training and test sets (here we'll try 80% train, 20% test)\n",
    "train_set, test_set = featuresets[:split_num], featuresets[split_num+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2c0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84daac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a classifier based on the training set\n",
    "# note the train_set is a list of tuples where the first item of the tuple is a dictionary of features\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cdf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(headline_features(\"Clevery Warns Tory Rebels: Don't Dump Truss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394271db",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(headline_features(\"Journalistâ€™s Takedown Of Government Excuses Is Brilliant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f60a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(headline_features(\"Do You Get More Anxious Or Sad In Autumn? There's A Reason For That\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba71f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f8816",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's improve this, let's try adding more keywords to our classifier, and remove number\n",
    "# and using nltk stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def headline_features2(headline):\n",
    "    headline = headline.lower()\n",
    "    words = headline.split()\n",
    "    s_words = stopwords.words('english')\n",
    "    keywords = [w for w in words if w not in s_words and not w.isdigit()]\n",
    "    while len(keywords) < 5:\n",
    "        keywords.append('None')\n",
    "    return {'first_keyword': keywords[0], 'second_keyword': keywords[1], 'third_keyword': keywords[2], 'fourth_keyword': keywords[3], 'fifth_keyword': keywords[4]}\n",
    "\n",
    "headline_features2('High Tech Works When It Enables High Touch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33626db",
   "metadata": {},
   "outputs": [],
   "source": [
    "zipped_features = zip(news.headline, news.category)\n",
    "featuresets = [(headline_features2(headline), category) for headline, category in zipped_features ]\n",
    "featuresets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dec11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35230788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "\n",
    "split_num = math.floor(len(featuresets)*.8)\n",
    "\n",
    "# split feature sets into training and test sets (here we'll try 80% train, 20% test)\n",
    "train_set, test_set = featuresets[:split_num], featuresets[split_num+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435051a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(headline_features(\"Alex Jones Will Likely Be Broke 'For The Rest Of His Life,' Ex-Prosecutor Says\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce0d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how else can we improve our classifier?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
